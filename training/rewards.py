##################################################################################
# Copyright (c) 2024 Matthew Thomas Beck                                         #
#                                                                                #
# All rights reserved. This code and its associated files may not be reproduced, #
# modified, distributed, or otherwise used, in part or in whole, by any person   #
# or entity without the express written permission of the copyright holder,      #
# Matthew Thomas Beck.                                                           #
##################################################################################





############################################################
############### IMPORT / CREATE DEPENDENCIES ###############
############################################################


########## IMPORT DEPENDENCIES ##########

##### import config #####

import utilities.config as config

##### import necessary libraries #####

import logging

##### import necessary functions #####

from training.orientation import track_orientation


########## CREATE DEPENDENCIES ##########

##### shared global variables #####

EPISODE_STEP = 0  # Legacy single-robot support
# Per-robot episode steps are now handled dynamically in training.py





################################################
############### REWARD FUNCTIONS ###############
################################################


########## CALCULATE REWARD ##########

def calculate_step_reward(commands, intensity, robot_id): # function to calculate reward for step

    ##### set variables #####

    global EPISODE_STEP

    # Get orientation for the specific robot
    center_pos, facing_deg = track_orientation(robot_id)
    if center_pos is None:
        return 0.0
    reward = 0.0
    was_perfect = True

    ##### fall detection #####

    current_height = center_pos[2]
    # Get robot-specific balance data
    if hasattr(track_orientation, 'robot_data') and robot_id in track_orientation.robot_data:
        current_balance = track_orientation.robot_data[robot_id].get('last_off_balance', 0.0)
    else:
        current_balance = 0.0
    has_fallen = current_balance > 90.0

    ##### reward balance #####
    
    balance_reward = _reward_balance(current_balance, robot_id)
    if balance_reward < 1.0:
        was_perfect = False

    reward += balance_reward

    ##### reward height #####

    height_reward = _reward_height(current_height, robot_id)
    if height_reward < 1.0:
        was_perfect = False

    reward += height_reward

    ##### reward rotation #####
    
    rotation_reward = _reward_rotation(track_orientation, commands, intensity, robot_id)
    if rotation_reward < 2.0:
        was_perfect = False
    
    reward += rotation_reward

    ##### reward movement #####

    movement_reward = _reward_movement(track_orientation, commands, intensity, robot_id)
    if movement_reward < 1.0:
        was_perfect = False
    
    reward += movement_reward

    ##### reward perfect execution ##### TODO fix me later, I am broken

    #if was_perfect and commands:
       # perfect_bonus = 10.0
        #reward += perfect_bonus
        #logging.debug(f"游끥 PERFECT EXECUTION! +{perfect_bonus:.1f} MASSIVE BONUS - All commands executed flawlessly!")
    #elif commands:
        #pass
        #logging.debug(f"游늵 Good execution, but not perfect - no bonus this time")

    ##### punish fall #####

    if has_fallen: # fall penalty
        fall_penalty = -100
        #logging.debug(f"Robot {robot_id} FELL - marking as inactive")
        
        # Mark robot as inactive (will be reset by main loop)
        import training.training as training_module
        if hasattr(training_module, 'agent_data') and training_module.agent_data:
            if robot_id in training_module.agent_data:
                training_module.agent_data[robot_id]['is_active'] = False
                #print(f"游뱄 Robot {robot_id} marked as INACTIVE (fell over)")

        return fall_penalty

    else: # survival bonus
        reward += 0.2

    ##### log reward #####
    
    #logging.debug(f"Robot {robot_id} reward: {reward:.1f}")
    return reward


########## BALANCE REWARD FUNCTION ##########

def _reward_balance(current_balance, robot_id): # function to reward balance

    ##### reward and range weights #####

    perfect_balance = 0.0
    terrible_balance = 90.0
    total_range = perfect_balance - terrible_balance
    balance_reward_magnitude = 1.0
    balance_penalty_magnitude = 1.0
    perfect_percentile = 10
    good_percentile = 20
    bad_percentile = 50
    terrible_percentile = 90

    ##### calculate ranges #####

    perfect_range = calculate_desired_value('balance', total_range, perfect_percentile, terrible_balance)
    good_range = calculate_desired_value('balance', total_range, good_percentile, terrible_balance)
    bad_range = calculate_desired_value('balance', total_range, bad_percentile, terrible_balance)
    terrible_range = calculate_desired_value('balance', total_range, terrible_percentile, terrible_balance)

    #logging.debug(f"Perfect range: {perfect_range:.1f}, Good range: {good_range:.1f}, Bad range: {bad_range:.1f}, Terrible range: {terrible_range:.1f}")

    ##### calculate balance reward #####

    if current_balance < good_range: # if in good range...
        if current_balance < perfect_range: # if perfect...
            balance_reward = balance_reward_magnitude
            #logging.debug(f"游댮 ROBOT {robot_id} PERFECT BALANCE: +{balance_reward:.1f}/{balance_reward_magnitude:.1f} reward - Balance: {current_balance:.1f}춿")
        else: # if good...
            balance_reward = (((100 / (perfect_range - good_range)) * (current_balance - good_range)) / 100) * balance_reward_magnitude
            #logging.debug(f"游 GOOD BALANCE: +{balance_reward:.2f}/{balance_reward_magnitude:.1f} reward - Balance: {current_balance:.1f}춿")

    elif current_balance > bad_range: # if in bad range...
        if current_balance > terrible_range: # if terrible...
            balance_reward = -balance_penalty_magnitude
            #logging.debug(f"游댯 TERRIBLE BALANCE: {balance_reward:.1f}/{balance_penalty_magnitude:.1f} penalty - Balance: {current_balance:.1f}춿")
        else: # if bad but not terrible...
            balance_reward_progress = (((100 / (bad_range - terrible_range)) * (current_balance - terrible_range)) / 100) * balance_penalty_magnitude
            balance_reward = -balance_penalty_magnitude + balance_reward_progress
            #logging.debug(f"游릭 POOR BALANCE: {balance_reward:.2f}/{balance_penalty_magnitude:.1f} penalty - Balance: {current_balance:.1f}춿")

    else: # if in middle ground...
        balance_reward = 0.0
        #logging.debug(f"游리 NEUTRAL BALANCE: No reward/penalty - Balance: {current_balance:.1f}춿")
    
    return balance_reward


########## HEIGHT REWARD FUNCTION ##########

def _reward_height(current_height, robot_id): # function to reward height

    ##### reward and range weights #####

    perfect_height = 0.129
    terrible_height = 0.043
    total_range = perfect_height - terrible_height
    height_reward_magnitude = 1.0
    height_penalty_magnitude = 1.0
    perfect_percentile = 10
    good_percentile = 20
    bad_percentile = 50
    terrible_percentile = 90

    ##### calculate ranges #####

    perfect_range = calculate_desired_value('height', total_range, perfect_percentile, terrible_height)
    good_range = calculate_desired_value('height', total_range, good_percentile, terrible_height)
    bad_range = calculate_desired_value('height', total_range, bad_percentile, terrible_height)
    terrible_range = calculate_desired_value('height', total_range, terrible_percentile, terrible_height)

    #logging.debug(f"Perfect range: {perfect_range:.3f}, Good range: {good_range:.3f}, Bad range: {bad_range:.3f}, Terrible range: {terrible_range:.3f}")

    ##### calculate height reward #####

    if current_height > good_range: # if in good range...
        if current_height > perfect_range: # if perfect...
            height_reward = height_reward_magnitude
            #logging.debug(f"游댮 ROBOT {robot_id} PERFECT HEIGHT: +{height_reward:.1f}/{height_reward_magnitude:.1f} reward - Height: {current_height:.3f}m")
        else: # if good...
            height_reward = (((100 / (perfect_range - good_range)) * (current_height - good_range)) / 100) * height_reward_magnitude
            #logging.debug(f"游 GOOD HEIGHT: +{height_reward:.2f}/{height_reward_magnitude:.1f} reward - Height: {current_height:.3f}m")

    elif current_height < bad_range: # if in bad range...
        if current_height < terrible_range: # if terrible...
            height_reward = -height_penalty_magnitude
            #logging.debug(f"游댯 TERRIBLE HEIGHT: {height_reward:.1f}/{height_penalty_magnitude:.1f} penalty - Height: {current_height:.3f}m")
        else: # if bad but not terrible...
            height_reward_progress = (((100 / (bad_range - terrible_range)) * (current_height - terrible_range)) / 100) * height_penalty_magnitude
            height_reward = -height_penalty_magnitude + height_reward_progress
            #logging.debug(f"游릭 POOR HEIGHT: {height_reward:.2f}/{height_penalty_magnitude:.1f} penalty - Height: {current_height:.3f}m")

    else: # if in middle ground...
        height_reward = 0.0
        #logging.debug(f"游리 NEUTRAL HEIGHT: No reward/penalty - Height: {current_height:.3f}m")
    
    return height_reward


########## ROTATION REWARD FUNCTIONS ##########

##### master reward function for rotation #####

def _reward_rotation(track_orientation, commands, intensity, robot_id): # function to reward rotation
    
    # Get robot-specific rotation data
    if (hasattr(track_orientation, 'robot_data') and 
        robot_id in track_orientation.robot_data and 
        'last_rotation' in track_orientation.robot_data[robot_id]): # if rotation data available...
        
        ##### check if rotation is commanded #####

        actual_rotation = track_orientation.robot_data[robot_id]['last_rotation']
        rotation_magnitude = abs(actual_rotation)
        command_list = commands.split('+') if isinstance(commands, str) else commands
        rotation_commanded = any(command in ['arrowleft', 'arrowright'] for command in command_list)
        specific_rotation_command = None
        if rotation_commanded:
            for command in command_list:
                if command in ['arrowleft', 'arrowright']:
                    specific_rotation_command = command
                    break

        ##### reward rotation or stability #####

        if rotation_commanded: # if robot supposed to be rotating...
            rotation_reward = _reward_rotation_direction(specific_rotation_command, actual_rotation, rotation_magnitude, robot_id)
        
        else: # if robot supposed to be standing still...
            rotation_reward = _reward_rotation_stability(rotation_magnitude, robot_id)

        return rotation_reward

    else: # if no rotation data available...
        return 0.0

##### child reward function for valid rotation #####

def _reward_rotation_direction(specific_rotation_command, actual_rotation, rotation_magnitude, robot_id): # function to reward valid rotation

    ##### reward and range weights #####

    perfect_rotation = 30.0
    terrible_rotation = 0.0
    total_range = perfect_rotation - terrible_rotation
    rotation_direction_reward_magnitude = 4.0
    rotation_direction_penalty_magnitude = 1.0
    perfect_percentile = 25
    quick_percentile = 50
    acceptable_percentile = 75
    slow_percentile = 100

    ##### calculate ranges #####

    perfect_range = calculate_desired_value('rotation', total_range, perfect_percentile, perfect_rotation)
    quick_range = calculate_desired_value('rotation', total_range, quick_percentile, perfect_rotation)
    acceptable_range = calculate_desired_value('rotation', total_range, acceptable_percentile, perfect_rotation)
    slow_range = calculate_desired_value('rotation', total_range, slow_percentile, perfect_rotation)

    #logging.warning(f"SUPPPOSED TO BE ROTATING {direction}!!!")
    #logging.debug(f"Perfect range: {perfect_range:.1f}, Quick range: {quick_range:.1f}, Acceptable range: {acceptable_range:.1f}, Slow range: {slow_range:.1f}")
    
    ##### determine rotation direction #####

    if specific_rotation_command == 'arrowleft': # if left rotation...
        direction = "LEFT"
        if actual_rotation > 0:
            correct_rotation_direction = True
        else:
            correct_rotation_direction = False
    elif specific_rotation_command == 'arrowright': # if right rotation...
        direction = "RIGHT"
        if actual_rotation < 0:
            correct_rotation_direction = True
        else:
            correct_rotation_direction = False
    else: # if rotation is not commanded or in the wrong direction...
        direction = "N/A"
        correct_rotation_direction = False

    ##### calculate rotation reward #####

    if correct_rotation_direction: # if rotation in correct direction...

        if rotation_magnitude > perfect_range:
            rotation_direction_reward = rotation_direction_reward_magnitude
            #logging.debug(f"游댮 PERFECT ROTATION MOVEMENT: +{rotation_direction_reward:.1f}/{rotation_direction_reward_magnitude:.1f} reward - Rotation: {rotation_magnitude:.1f}춿")
        elif rotation_magnitude > quick_range:
            rotation_direction_reward = (((100 / perfect_range) * rotation_magnitude) / 100) * rotation_direction_reward_magnitude
            #logging.debug(f"游 QUICK ROTATION MOVEMENT: +{rotation_direction_reward:.2f}/{rotation_direction_reward_magnitude:.1f} reward - Rotation: {rotation_magnitude:.1f}춿")
        elif rotation_magnitude > acceptable_range:
            rotation_direction_reward = (((100 / perfect_range) * rotation_magnitude) / 100) * rotation_direction_reward_magnitude
            #logging.debug(f"游리 ACCEPTABLE ROTATION MOVEMENT: +{rotation_direction_reward:.2f}/{rotation_direction_reward_magnitude:.1f} reward - Rotation: {rotation_magnitude:.1f}춿")
        else:
            rotation_direction_reward = (((100 / perfect_range) * rotation_magnitude) / 100) * rotation_direction_reward_magnitude
            #logging.debug(f"游릭 SLOW ROTATION MOVEMENT: +{rotation_direction_reward:.1f}/{rotation_direction_reward_magnitude:.1f} reward - Rotation: {rotation_magnitude:.1f}춿")

    else: # if rotation in wrong direction...
        rotation_direction_reward = -rotation_direction_penalty_magnitude
        #logging.debug(f"游댯 WRONG ROTATION DIRECTION: {rotation_direction_reward:.1f}/{rotation_direction_penalty_magnitude:.1f} penalty - Expected: {actual_rotation}, Got: {actual_rotation:.1f}춿, Rotation: {rotation_magnitude:.1f}춿")

    return rotation_direction_reward

##### child reward function for no rotation #####

def _reward_rotation_stability(rotation_magnitude, robot_id): # function to reward no rotation

    ##### reward and range weights #####

    perfect_rotation = 0.0
    terrible_rotation = 30.0
    total_range = perfect_rotation - terrible_rotation
    rotation_stability_reward_magnitude = 1.0
    rotation_stability_penalty_magnitude = 1.0
    perfect_percentile = 10
    good_percentile = 20
    bad_percentile = 50
    terrible_percentile = 90
    
    ##### calculate ranges #####

    perfect_range = calculate_desired_value('no_rotation', total_range, perfect_percentile, terrible_rotation)
    good_range = calculate_desired_value('no_rotation', total_range, good_percentile, terrible_rotation)
    bad_range = calculate_desired_value('no_rotation', total_range, bad_percentile, terrible_rotation)
    terrible_range = calculate_desired_value('no_rotation', total_range, terrible_percentile, terrible_rotation)

    #logging.warning("SUPPPOSED TO BE STILL!!!")
    #logging.debug(f"Perfect range: {perfect_range:.1f}, Good range: {good_range:.1f}, Bad range: {bad_range:.1f}, Terrible range: {terrible_range:.1f}")

    ##### calculate no rotation reward #####

    if rotation_magnitude < good_range:
        if rotation_magnitude < perfect_range:
            rotation_stability_reward = rotation_stability_reward_magnitude
            #logging.debug(f"游댮 ROBOT {robot_id} PERFECT ROTATION STABILITY: +{rotation_stability_reward:.1f}/{rotation_stability_reward_magnitude:.1f} reward - Rotation: {rotation_magnitude:.1f}춿")
        else:
            rotation_stability_reward = (((100 / (good_range - perfect_range)) * (rotation_magnitude - perfect_range)) / 100) * rotation_stability_reward_magnitude
            #logging.debug(f"游 GOOD ROTATION STABILITY: +{rotation_stability_reward:.2f}/{rotation_stability_reward_magnitude:.1f} reward - Rotation: {rotation_magnitude:.1f}춿")

    elif rotation_magnitude > bad_range:
        if rotation_magnitude > terrible_range:
            rotation_stability_reward = -rotation_stability_penalty_magnitude
            #logging.debug(f"游댯 TERRIBLE ROTATION STABILITY: {rotation_stability_reward:.1f}/{rotation_stability_penalty_magnitude:.1f} penalty - Rotation: {rotation_magnitude:.1f}춿")
        else:
            rotation_stability_reward_progress = (((100 / (bad_range - terrible_range)) * (rotation_magnitude - terrible_range)) / 100) * rotation_stability_penalty_magnitude
            rotation_stability_reward = -rotation_stability_penalty_magnitude + rotation_stability_reward_progress
            #logging.debug(f"游릭 POOR ROTATION STABILITY: {rotation_stability_reward:.2f}/{rotation_stability_penalty_magnitude:.1f} penalty - Rotation: {rotation_magnitude:.1f}춿")

    else: # if in middle ground...
        rotation_stability_reward = 0.0
        #logging.debug(f"游리 NEUTRAL ROTATION STABILITY: No reward/penalty - Rotation: {rotation_magnitude:.1f}춿")

    return rotation_stability_reward


########## MOVEMENT REWARD FUNCTIONS ##########

##### master reward fucntion for movement #####

def _reward_movement(track_orientation, commands, intensity, robot_id): # function to reward movement

    movement_reward = 0.0 # initialize movement reward

    # Get robot-specific movement data
    if (hasattr(track_orientation, 'robot_data') and 
        robot_id in track_orientation.robot_data and 
        'last_movement_data' in track_orientation.robot_data[robot_id]): # if movement data available...
        
        try: # attempt to get movement data...

            ##### set orientation and movement variables #####

            movement_data = track_orientation.robot_data[robot_id]['last_movement_data']
            total_displacement = {
                'w': movement_data.get('w'),
                's': movement_data.get('s'),
                'a': movement_data.get('a'),
                'd': movement_data.get('d')
            }
            command_list = commands.split('+') if isinstance(commands, str) else commands

            # if there are commands and any valid 'wasd' in command list...
            if commands and any(command in ['w', 'a', 's', 'd'] for command in command_list):
                
                # remove 'arrowleft' and 'arrowright' from command list
                filtered_commands = [command for command in command_list if command not in ['arrowleft', 'arrowright']]

                if len(filtered_commands) > 1: # if multidirectional command (i.e. 'w+a')...

                    for command in filtered_commands: # loop through each command
                        movement_reward += _reward_movement_direction(command, total_displacement, intensity, True)

                    return movement_reward

                else: # if unidirectional command (i.e. 'w')...
                    command = filtered_commands[0]  # Get the single command
                    movement_reward += _reward_movement_direction(command, total_displacement, intensity, False)
                    return movement_reward

            else: # if robot is supposed to be still...
                movement_reward += _reward_movement_stillness(total_displacement)
                return movement_reward

        except Exception as e: # if movement analysis failed...
            return movement_reward

    else: # if no movement data available...
        return movement_reward

##### child reward function for valid movement #####

def _reward_movement_direction(command, total_displacement, intensity, multidirectional, robot_id): # function to reward valid movement

    ##### set variables #####

    movement_direction_reward = 0.0 # initialize direction reward
    commanded_displacement = total_displacement.get(command, 0) # displacement in commanded direction
    
    ##### get perpendicular displacements for drift calculation #####

    if command == 'w' or command == 's': # if forward or backward movement...
        perpendicular_displacement = abs(total_displacement.get('a', 0)) + abs(total_displacement.get('d', 0))
    else: # if left or right movement...
        perpendicular_displacement = abs(total_displacement.get('w', 0)) + abs(total_displacement.get('s', 0))

    ##### reward and range weights #####

    perfect_movement = 0.1 # 10cm of movement
    terrible_movement = 0.0
    acceptable_drift = 0.05 # 5cm of drift allowed
    total_range = perfect_movement - terrible_movement
    movement_reward_magnitude = 4.0
    movement_penalty_magnitude = 1.0
    perfect_percentile = 25
    good_percentile = 50
    acceptable_percentile = 75
    slow_percentile = 100
    
    ##### calculate ranges #####

    perfect_range = calculate_desired_value('rotation', total_range, perfect_percentile, perfect_movement)
    good_range = calculate_desired_value('rotation', total_range, good_percentile, perfect_movement)
    acceptable_range = calculate_desired_value('rotation', total_range, acceptable_percentile, perfect_movement)
    slow_range = calculate_desired_value('rotation', total_range, slow_percentile, perfect_movement)

    #logging.warning(f"SUPPPOSED TO BE MOVING IN {command.upper()} DIRECTION!!!")
    #logging.debug(f"Perfect range: {perfect_range:.3f}, Good range: {good_range:.3f}, Acceptable range: {acceptable_range:.3f}, Slow range: {slow_range:.3f}")

    ##### calculate movement reward #####

    if commanded_displacement > 0: # if moving in correct direction...
        if commanded_displacement > perfect_range: # if perfect...
            movement_direction_reward = movement_reward_magnitude
            #logging.debug(f"游댮 ROBOT {robot_id} PERFECT '{command.upper()}' MOVEMENT: +{movement_direction_reward:.1f}/{movement_reward_magnitude:.1f} reward - Displacement: {commanded_displacement:.3f}m")
        elif commanded_displacement > good_range: # if good...
            movement_direction_reward = (((100 / perfect_range) * commanded_displacement) / 100) * movement_reward_magnitude
            #logging.debug(f"游 QUICK '{command.upper()}' MOVEMENT: +{movement_direction_reward:.2f}/{movement_reward_magnitude:.1f} reward - Displacement: {commanded_displacement:.3f}m")
        elif commanded_displacement > acceptable_range: # if acceptable...
            movement_direction_reward = (((100 / perfect_range) * commanded_displacement) / 100) * movement_reward_magnitude
            #logging.debug(f"游리 ACCEPTABLE '{command.upper()}' MOVEMENT: +{movement_direction_reward:.2f}/{movement_reward_magnitude:.1f} reward - Displacement: {commanded_displacement:.3f}m")
        else: # if slow...
            movement_direction_reward = (((100 / perfect_range) * commanded_displacement) / 100) * movement_reward_magnitude
            #logging.debug(f"游릭 SLOW '{command.upper()}' MOVEMENT: +{movement_direction_reward:.1f}/{movement_reward_magnitude:.1f} reward - Displacement: {commanded_displacement:.3f}m")
        
        # if unidirectional and significant perpendicular movement...
        if not multidirectional and perpendicular_displacement > acceptable_drift:
            drift_penalty = min(perpendicular_displacement * 10, movement_penalty_magnitude)
            movement_direction_reward -= drift_penalty
            #logging.debug(f"丘멆잺 DRIFT PENALTY: -{drift_penalty:.2f} for {perpendicular_displacement:.3f}m perpendicular movement")
            
    else: # if not moving in correct direction...
        movement_direction_reward = -movement_penalty_magnitude
        #logging.debug(f"游댯 NO '{command.upper()}' MOVEMENT: {movement_direction_reward:.1f}/{movement_penalty_magnitude:.1f} penalty - Displacement: {commanded_displacement:.3f}m")

    return movement_direction_reward

##### child reward function for no movement #####

def _reward_movement_stillness(total_displacement, robot_id): # function to reward stillness

    ##### reward and range weights #####

    perfect_movement = 0.0
    terrible_movement = 0.1 # 10cm of movement
    total_range = terrible_movement - perfect_movement
    movement_stillness_reward_magnitude = 1.0
    movement_stillness_penalty_magnitude = 1.0
    perfect_percentile = 10
    good_percentile = 20
    bad_percentile = 50
    terrible_percentile = 90
    
    ##### calculate total movement magnitude #####
    
    total_movement = abs(total_displacement.get('w', 0)) + abs(total_displacement.get('s', 0)) + \
                     abs(total_displacement.get('a', 0)) + abs(total_displacement.get('d', 0))
    
    ##### calculate ranges #####

    perfect_range = calculate_desired_value('no_movement', total_range, perfect_percentile, perfect_movement)
    good_range = calculate_desired_value('no_movement', total_range, good_percentile, perfect_movement)
    bad_range = calculate_desired_value('no_movement', total_range, bad_percentile, perfect_movement)
    terrible_range = calculate_desired_value('no_movement', total_range, terrible_percentile, perfect_movement)

    #logging.warning("SUPPPOSED TO BE STILL!!!")
    #logging.debug(f"Perfect range: {perfect_range:.3f}, Good range: {good_range:.3f}, Bad range: {bad_range:.3f}, Terrible range: {terrible_range:.3f}")

    ##### calculate no movement reward #####

    if total_movement < good_range:
        if total_movement < perfect_range:
            movement_stillness_reward = movement_stillness_reward_magnitude
            #logging.debug(f"游댮 ROBOT {robot_id} PERFECT MOVEMENT STILLNESS: +{movement_stillness_reward:.1f}/{movement_stillness_reward_magnitude:.1f} reward - Movement: {total_movement:.3f}m")
        else:
            movement_stillness_reward = (((100 / (good_range - perfect_range)) * (total_movement - perfect_range)) / 100) * movement_stillness_reward_magnitude
            #logging.debug(f"游 GOOD MOVEMENT STILLNESS: +{movement_stillness_reward:.2f}/{movement_stillness_reward_magnitude:.1f} reward - Movement: {total_movement:.3f}m")

    elif total_movement > bad_range:
        if total_movement > terrible_range:
            movement_stillness_reward = -movement_stillness_penalty_magnitude
            #logging.debug(f"游댯 TERRIBLE MOVEMENT STILLNESS: {movement_stillness_reward:.1f}/{movement_stillness_penalty_magnitude:.1f} penalty - Movement: {total_movement:.3f}m")
        else:
            movement_progress = (((100 / (bad_range - terrible_range)) * (total_movement - terrible_range)) / 100) * movement_stillness_penalty_magnitude
            movement_stillness_reward = -movement_stillness_penalty_magnitude + movement_progress
            #logging.debug(f"游릭 POOR MOVEMENT STILLNESS: {movement_stillness_reward:.2f}/{movement_stillness_penalty_magnitude:.1f} penalty - Movement: {total_movement:.3f}m")

    else: # if in middle ground...
        movement_stillness_reward = 0.0
        #logging.debug(f"游리 NEUTRAL MOVEMENT STILLNESS: No reward/penalty - Movement: {total_movement:.3f}m")

    return movement_stillness_reward


########## REWARD RANGE CALCULATION FUNCTION ##########

def calculate_desired_value(reward_type, total_range, selected_range, special_value): # function to calculate desired figure for reward calculation

    if reward_type == 'balance' or reward_type == 'no_rotation': # perfect value is 0, terrible value greater than 0

        desired_value = (((-1 * total_range) / 100) * selected_range)

    elif reward_type == 'height': # the numbers are small so floating points are being an issue

        scale_factor = 1000000
        total_range = total_range * scale_factor
        special_value = special_value * scale_factor

        desired_value = ((total_range / 100) * (100 - selected_range) + special_value) / scale_factor

    elif reward_type == 'rotation': # degrees from any value greater than 0 to 0
        desired_value = special_value - (((total_range) / 100) * selected_range)

    elif reward_type == 'no_movement':
        desired_value = ((total_range / 100) * selected_range)

    return desired_value
